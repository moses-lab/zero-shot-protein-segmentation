{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ami G. Sangster \n",
    "\n",
    "!git clone https://github.com/moses-lab/zero-shot-protein-segmentation.git\n",
    "\n",
    "import sys\n",
    "working_path = '/content/zero-shot-protein-segmentation/'\n",
    "sys.path.insert(0,working_path)\n",
    "\n",
    "import csv\n",
    "import h5py\n",
    "from protT5_embedder import get_embeddings\n",
    "from functions import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reccommended: don't run the blocks of code that save the embeddings unless \n",
    "#               you actually want to have them, these files can be very large\n",
    "#               (per-residue embeddings of the human proteome = 22GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get per-residue protien embeddings from prot_t5_xl_half_uniref50-enc\n",
    "# note: colab may temporarily ban you if you use too much GPU time\n",
    "\n",
    "# emb_dict is a dictionary where the keys are UniProt protein IDs (as given in the fasta file)\n",
    "# the values of the dictionary are data matrices that contain the per-residue embedding from prot_t5_xl_half_uniref50-enc\n",
    "emb_dict = get_embeddings(seq_path=\"protein_sequences_demo.fasta\", model_dir=\"\", \n",
    "                          per_protein=False, max_residues=4000, max_seq_len=4000, max_batch=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save per-residue embeddings for whole proteins\n",
    "\n",
    "# note: files saved in colab must be downloaded to your desktop before the session is closed.\n",
    "# you can find the file on the left hand side. Click on the file folder and it should be listed there.\n",
    "\n",
    "whole_emb_path = \"ProtT5_whole_protein_embeddings.hdf5\"\n",
    "\n",
    "with h5py.File(str(whole_emb_path), \"a\") as hf:\n",
    "    for sequence_id, embedding in emb_dict.items():\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        hf.create_dataset(sequence_id, data=embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment the protein embeddings using change point analysis\n",
    "\n",
    "# protein_segment_boundaries is a dictionary where the keys are UniProt protein IDs (as given in the fasta file)\n",
    "# the values of the dictionary are a list of boundaries between protein segments\n",
    "protein_segment_boundaries = get_protein_segment_boundaries(emb_dict, max_bkps_per100aa=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save segment boundaries\n",
    "\n",
    "# note: files saved in colab must be downloaded to your desktop before the session is closed.\n",
    "# you can find the file on the left hand side. Click on the file folder and it should be listed there.\n",
    "\n",
    "seg_bounds_path = \"ZPS_segment_boundaries.tsv\"\n",
    "\n",
    "with open(seg_bounds_path, 'w', newline='') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
    "    for protein_id, protein_seg in protein_segment_boundaries.items():\n",
    "        writer.writerow([protein_id, protein_seg])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate segment embeddings\n",
    "\n",
    "# protein_segment_embeddings is a dictionary where the keys are \"ID start-stop\"\n",
    "# where ID is the UniProt protein ID, start and stop are the start and stop positions \n",
    "# of the protein segment (defined in above block of code) in zero-based indexing\n",
    "# the values of the dictionary are 1x1024 data vector containing the segment embedding\n",
    "protein_segment_embeddings = get_protein_segment_embeddings(emb_dict, protein_segment_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save segment embeddings\n",
    "\n",
    "# note: files saved in colab must be downloaded to your desktop before the session is closed.\n",
    "# you can find the file on the left hand side. Click on the file folder and it should be listed there.\n",
    "\n",
    "seg_emb_path = \"ZPS_segment_embeddings.hdf5\"\n",
    "\n",
    "with h5py.File(str(seg_emb_path), \"a\") as hf:\n",
    "    for sequence_key, embedding in protein_segment_embeddings.items():\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        hf.create_dataset(sequence_key, data=embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
